{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03470cb-6b7e-4cb8-81c6-464596e25222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, timedelta\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "from glicko2 import Glicko2\n",
    "from trueskill import TrueSkill\n",
    "from scipy.stats import norm\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 16\n",
    "plt.rcParams[\"xtick.labelsize\"] = 16\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "plt.rcParams[\"lines.linewidth\"] = 3\n",
    "plt.rcParams[\"figure.titlesize\"] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfee22-3df0-41e7-a4a5-53c136b4329e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1013836-04d6-493d-870d-37ff5f6b1930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_preprocessed.csv', parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320066c5-29c3-4548-b120-a0c3ecc8669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c11b5e-1d68-4bfc-abb6-db0171e65c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dictionary lookup with team id and the day before the team first played. this is the day that we will set the initial rating on\n",
    "\n",
    "teams = pd.melt(df[['winner', 'loser', 'Date']], value_vars=['winner', 'loser'], id_vars='Date', value_name='team', var_name='status')\n",
    "all_teams = teams.team.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceee26d-a3b2-4566-8b6f-6b0483e62043",
   "metadata": {},
   "source": [
    "## Brier Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd9e38-1712-44d6-9515-844095541b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def brier_score(preds: list, outs: list) -> float:\n",
    "    return np.average((outs - preds) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f7014-105c-493e-bb68-c12173c24fef",
   "metadata": {},
   "source": [
    "## Glicko-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a068d05-7083-4c98-99e0-bca99c039239",
   "metadata": {},
   "source": [
    "#### pseudocode\n",
    "\n",
    "    for period_name, series in df.groupby(period):\n",
    "        provisional_ratings = {}\n",
    "        for team in all_teams:\n",
    "            - get all matches the team has played in this series\n",
    "            - if so, rewrite team results in the format taken by algorithm\n",
    "            - calculate new provisional rating of the team using this series and save it\n",
    "        after doing this for all teams, the provisional ratings you calculated become the real ratings of the teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6d27e4-dd10-41f6-925c-d4f9cdb0a7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_provisional_ratings(period, group, ratings, alg):\n",
    "    # create the rating for each team after the rating period\n",
    "    prov_ratings = {}\n",
    "    # get most recent ratings of each team, or generate default ones\n",
    "    old_ratings = {}\n",
    "    for team in all_teams:\n",
    "        try:\n",
    "            old_ratings[team] = ratings[team]['rating'][-1]\n",
    "        except IndexError:\n",
    "            old_ratings[team] = alg.create_rating()\n",
    "            \n",
    "    # now generate the series for the teams\n",
    "    # each series is a list of tuples (outcome, opponent_rating)\n",
    "    series = defaultdict(list)\n",
    "    for row in group.itertuples():\n",
    "        # temporarily save this rating\n",
    "        if row.draw:\n",
    "            series[row.winner].append((0.5, old_ratings[row.loser]))\n",
    "            series[row.loser].append((0.5, old_ratings[row.winner]))\n",
    "        else:    \n",
    "            series[row.winner].append((1, old_ratings[row.loser]))\n",
    "            series[row.loser].append((0, old_ratings[row.winner]))\n",
    "    \n",
    "    # pass the rating before the series and the sereis to the algorithm to get updated rating\n",
    "    for team in all_teams:\n",
    "        prov_ratings[team] = alg.rate(old_ratings[team], series[team])\n",
    "    return prov_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37968dd4-66b2-491f-a844-95c5476a0ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_glicko(alg, series_col_name='Season'):\n",
    "    # Function to generate Glicko ratings using the dataframe\n",
    "    # initialize ratings dictionary\n",
    "    ratings = dict()\n",
    "    for t in all_teams:\n",
    "        ratings[t] = {'date': [], 'rating': []}\n",
    "    for period, group in df.groupby(series_col_name):\n",
    "        provisional_ratings = get_provisional_ratings(period, group, ratings, alg=glicko)\n",
    "        for team, r in provisional_ratings.items():\n",
    "            ratings[team]['rating'].append(r)\n",
    "            ratings[team]['date'].append(period)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf3cc47-43c9-45af-9639-19a1b7aab5d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We initialize the algorithm here and not in the function because we will need it later for win predictions\n",
    "mu=1500\n",
    "phi=350\n",
    "tau=.2\n",
    "sigma=.06\n",
    "glicko = Glicko2(mu=mu, phi=phi, tau=tau, sigma=sigma)\n",
    "\n",
    "# Try using Day, Month, different parameter values... You might need to create new columns first\n",
    "ratings = rate_glicko(alg=glicko)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3997c9cd-d8e5-44d9-82b7-6c07e678941d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plotting time: let's look at some teams over time!\n",
    "team = 'einfrankfurt'\n",
    "fig, ax = plt.subplots(2,1, figsize=(15,5), sharex=True)\n",
    "ax[0].plot(ratings[team]['date'], [i.mu for i in ratings[team]['rating']], '-o')\n",
    "ax[0].set_title('mu')\n",
    "ax[1].plot(ratings[team]['date'], [i.phi for i in ratings[team]['rating']], '-o')\n",
    "ax[1].set_title('phi')\n",
    "plt.xticks(rotation=45)\n",
    "_ = plt.suptitle(f'{team} glicko ratings')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c016e61-77e3-4b9a-92b1-8344a312f7ac",
   "metadata": {},
   "source": [
    "- How can we compare this to Elo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9ed7c3-c370-476f-99fa-00e38fb7c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = rate_glicko(alg=glicko, series_col_name='Date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0f51a6-3983-44b3-ab2a-2651eb74aec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "team = 'einfrankfurt'\n",
    "fig, ax = plt.subplots(2,1, figsize=(15,5), sharex=True)\n",
    "ax[0].plot(ratings[team]['date'], [i.mu for i in ratings[team]['rating']])\n",
    "ax[0].set_title('mu')\n",
    "ax[1].plot(ratings[team]['date'], [i.phi for i in ratings[team]['rating']])\n",
    "ax[1].set_title('phi')\n",
    "plt.xticks(rotation=45)\n",
    "_ = plt.suptitle(f'{team} glicko ratings')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0811e-1581-4717-84c6-d5cdf2322405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# confidence intervals\n",
    "team = 'einfrankfurt'\n",
    "plt.figure(figsize=(15,3))\n",
    "\n",
    "#plt.plot(match_dates[team], elo_30_ratings[team][:-1], label='elo 32')\n",
    "plt.errorbar(ratings[team]['date'], [i.mu for i in ratings[team]['rating']], yerr=[i.phi for i in ratings[team]['rating']], errorevery=1, ecolor='b', color='r')\n",
    "\n",
    "_ = plt.title(team + ' ratings with confidence intervals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188ef5c-447d-445f-9388-b285ccb13cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which period do you want to use as series? Day, Week, Month...\n",
    "# remember to have provisional ratings within the rating period and only update the \"real rating\" after the series end!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90169e6c-f399-407c-9ff4-a23d40f1d8fb",
   "metadata": {},
   "source": [
    "## Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eee7ae-0adc-4e38-a3ba-ec06aa0354ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_on_date(rating_dict, team, date):\n",
    "    # todo: how do we manage ratings during calibration period?\n",
    "    date_idx = rating_dict[team]['date'].index(date)\n",
    "    if date_idx >= 1:\n",
    "        return rating_dict[team]['rating'][date_idx - 1]\n",
    "    else:\n",
    "        return glicko.create_rating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6055dbcc-1dee-4627-8fe8-0472ac2224b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the lowest confidence interval mu - 2*RD\n",
    "def apply_confidence_glicko(glicko_ratings: list) -> np.array:\n",
    "    skill = []\n",
    "    for r in glicko_ratings:\n",
    "        skill.append(r.mu - 2*r.phi)\n",
    "    return np.array(skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d9e32-bc24-45d1-bde2-fb721c339751",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7f4f1-d00b-4a4f-8dd2-32ce408d3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# win predictions - we need to scale down to Glicko 1 and then calculate win probability\n",
    "def predict_glicko_winner(glicko, r1, r2) -> float:\n",
    "    r1_g2 = glicko.scale_down(r1)\n",
    "    r2_g2 = glicko.scale_down(r2)\n",
    "    # equivalent to calling Glicko2.reduce_impact\n",
    "    g = 1 / np.sqrt(1+3*r1_g2.phi **2 / np.pi**2)\n",
    "    # equivalent to calling Glicko2.expect_score\n",
    "    E = 1 / (1 + np.exp(-g * (r1_g2.mu - r2_g2.mu)))\n",
    "    return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa77a1f-19c7-4470-b0f5-78d8e5d1f706",
   "metadata": {},
   "outputs": [],
   "source": [
    "glicko_predictions = []\n",
    "for winner, loser, date in zip(df.winner.values, df.loser.values, df.Date.values):\n",
    "    winner_rating = get_rating_on_date(ratings, winner, date)\n",
    "    loser_rating = get_rating_on_date(ratings, loser, date)\n",
    "    glicko_predictions.append(predict_glicko_winner(glicko, winner_rating, loser_rating))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b287d6f2-4ecd-4766-a6d9-00632c0947b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_score(glicko_predictions, np.ones(len(glicko_predictions))-0.5*df.draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0ce65-4705-4d42-bee5-92baa9284a4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalift-workshop",
   "language": "python",
   "name": "datalift-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
