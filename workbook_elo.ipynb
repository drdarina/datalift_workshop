{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03470cb-6b7e-4cb8-81c6-464596e25222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, timedelta\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "from glicko2 import Glicko2\n",
    "from trueskill import TrueSkill\n",
    "from scipy.stats import norm\n",
    "from collections import defaultdict\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams[\"figure.figsize\"] = (15,5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "plt.rcParams[\"axes.titlesize\"] = 20\n",
    "plt.rcParams[\"ytick.labelsize\"] = 16\n",
    "plt.rcParams[\"xtick.labelsize\"] = 16\n",
    "plt.rcParams[\"axes.labelsize\"] = 16\n",
    "plt.rcParams[\"lines.linewidth\"] = 3\n",
    "plt.rcParams[\"figure.titlesize\"] = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfee22-3df0-41e7-a4a5-53c136b4329e",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1013836-04d6-493d-870d-37ff5f6b1930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_preprocessed.csv', parse_dates=['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c11b5e-1d68-4bfc-abb6-db0171e65c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a dictionary lookup with team id and the day before the team first played. this is the day that we will set the initial rating on\n",
    "\n",
    "teams = pd.melt(df[['winner', 'loser', 'Date']], value_vars=['winner', 'loser'], id_vars='Date', value_name='team', var_name='status')\n",
    "all_teams = teams.team.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d029df-ce69-4974-8018-488725485cdf",
   "metadata": {},
   "source": [
    "## Elo\n",
    "\n",
    "Win probability:\n",
    "\n",
    "$$E_A = \\frac{1}{1+10^{\\frac{R_A-R_B}{n}}}$$\n",
    "\n",
    "Update Rule:\n",
    "\n",
    "$$R'_A = R_A + K(S_A-E_A)$$\n",
    "\n",
    "Outcomes are:\n",
    "- 1 for win\n",
    "- 0 for loss\n",
    "- 0.5 wor draw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39160849-59a7-439d-857a-c0b330709d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write two functions: one that calculates the expected win probability and one that updates the rating\n",
    "# remember you also need an initial rating value\n",
    "\n",
    "def expected(a, b, n=400) -> float:\n",
    "    return \n",
    "\n",
    "def update(current, expected, outcome, k=10) -> float:\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4be908-3ae5-4333-a06e-c12053c95e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_elo(n=400, k=10, init_rating=1200):\n",
    "    # set up rating lookup\n",
    "    ratings = dict()\n",
    "    for t in all_teams:\n",
    "        ratings[t] = {'date': [], 'rating': []}\n",
    "    # iterate over dataframe and assign ratings\n",
    "    for row in df.itertuples():\n",
    "        loser = row.loser\n",
    "        winner = row.winner\n",
    "        date = row.Date\n",
    "        # use default rating if the team shows up for the first time\n",
    "        try:\n",
    "            old_rating_winner = ratings[winner]['rating'][-1]\n",
    "        except IndexError:\n",
    "            old_rating_winner = init_rating\n",
    "        try:\n",
    "            old_rating_loser = ratings[loser]['rating'][-1]\n",
    "        except IndexError:\n",
    "            old_rating_loser = init_rating\n",
    "        exp_winner = expected(old_rating_winner, old_rating_loser, n)\n",
    "        exp_loser = expected(old_rating_loser, old_rating_winner, n)\n",
    "        if row.draw:\n",
    "            rating_winner = update(old_rating_winner, exp_winner, 0.5, k)\n",
    "            rating_loser = update(old_rating_loser, exp_loser, 0.5, k)\n",
    "        else:\n",
    "            rating_winner = update(old_rating_winner, exp_winner, 1, k)\n",
    "            rating_loser = update(old_rating_loser, exp_loser, 0, k)\n",
    "        ratings[winner]['rating'].append(rating_winner)\n",
    "        ratings[winner]['date'].append(date)        \n",
    "        ratings[loser]['rating'].append(rating_loser)\n",
    "        ratings[loser]['date'].append(date)\n",
    "    return ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8222dc-e2de-4fb1-a73d-f030bf94b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting time: let's look at some teams over time!\n",
    "team1 = 'einfrankfurt'\n",
    "team2 = 'bayernmunich'\n",
    "ratings = rate_elo(n=400, k=10, init_rating=1200)\n",
    "fig, ax = plt.subplots()\n",
    "plt.plot(ratings[team1]['date'], ratings[team1]['rating'], label=team1)\n",
    "plt.plot(ratings[team2]['date'], ratings[team2]['rating'], label=team2)\n",
    "plt.legend()\n",
    "_ = plt.title('elo ratings (after match)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea42e87-e81c-4567-8df7-b26092ff93e0",
   "metadata": {},
   "source": [
    "## Testing Elo quality\n",
    "\n",
    "- We don't need a test set\n",
    "- Careful about calibration time\n",
    "\n",
    "Brier score:\n",
    "\n",
    "$$BS = \\frac{1} {N} \\sum_{i=1}^N(E_i-S_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd9e38-1712-44d6-9515-844095541b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for the brier score\n",
    "\n",
    "# could also use the one in sklearn.metrics\n",
    "def brier_score(preds: list, outs: list) -> float:\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01eee7ae-0adc-4e38-a3ba-ec06aa0354ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rating_on_date(rating_dict, team, date, init_rating=1200):\n",
    "    # todo: how do we manage ratings during calibration period?\n",
    "    # since we stored the ratings AFTER the match, we always need to use the one before that day\n",
    "    date_idx = rating_dict[team]['date'].index(date)\n",
    "    if date_idx >= 1:\n",
    "        return rating_dict[team]['rating'][date_idx - 1]\n",
    "    else:\n",
    "        return init_rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec22666-f47f-442b-974e-cd6b8479f992",
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_predictions = []\n",
    "for winner, loser, date in zip(df.winner.values, df.loser.values, df.Date.values):\n",
    "    winner_rating = get_rating_on_date(ratings, winner, date)\n",
    "    loser_rating = get_rating_on_date(ratings, loser, date)\n",
    "    elo_predictions.append(expected(winner_rating, loser_rating, n=400))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e73f73-9a66-4671-ae33-9f9276ceb504",
   "metadata": {},
   "outputs": [],
   "source": [
    "brier_score(elo_predictions, np.ones(len(elo_predictions))-0.5*df.draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ad4a8-7d2b-40c0-9f0e-74d2eaa2cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning time!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalift-workshop",
   "language": "python",
   "name": "datalift-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
