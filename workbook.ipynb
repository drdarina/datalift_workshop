{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03470cb-6b7e-4cb8-81c6-464596e25222",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import date, timedelta\n",
    "import pickle\n",
    "import requests\n",
    "import numpy as np\n",
    "from glicko2 import Glicko2\n",
    "from trueskill import TrueSkill\n",
    "from scipy.stats import norm\n",
    "from collections import defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1013836-04d6-493d-870d-37ff5f6b1930",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfee22-3df0-41e7-a4a5-53c136b4329e",
   "metadata": {},
   "source": [
    "## First look at the data\n",
    "\n",
    "This is real Counter-Strike match data.\n",
    "\n",
    "`perid` is the Bayes Esports identifier\n",
    "\n",
    "- Take a look at the available data points\n",
    "- Check data integrity\n",
    "- How would you restructure it? (Hint: Think about the possible usecases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6e161f-e30d-4724-8f4e-cd331fc9907d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffa863b-4091-4bfb-bfd6-219309ce1975",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48d029df-ce69-4974-8018-488725485cdf",
   "metadata": {},
   "source": [
    "## Elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39160849-59a7-439d-857a-c0b330709d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write two functions: one that calculates the expected win probability and one that updates the rating\n",
    "# remember you also need an initial rating value\n",
    "\n",
    "def expected() -> float:\n",
    "    pass\n",
    "\n",
    "def update() -> float:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af5d3b-05b5-43f0-a679-54b16029c61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's apply that function to the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8222dc-e2de-4fb1-a73d-f030bf94b1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting time: let's look at the team with the most ratings in our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea42e87-e81c-4567-8df7-b26092ff93e0",
   "metadata": {},
   "source": [
    "## Testing Elo quality\n",
    "\n",
    "- We don't need a test set\n",
    "- Careful about calibration time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd9e38-1712-44d6-9515-844095541b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write a function for the brier score\n",
    "\n",
    "def brier() -> float:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42f3698-99cd-4779-a0f3-1540c808ded5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you want to use the elo scores BEFORE the match to predict the outcome, then compare to the actual outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141ad4a8-7d2b-40c0-9f0e-74d2eaa2cf02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122f7014-105c-493e-bb68-c12173c24fef",
   "metadata": {},
   "source": [
    "## Glicko-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5c6c4a-5342-40db-885b-e281cbb3dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters to play around with\n",
    "tau = .2\n",
    "sigma = .06\n",
    "mu = 1500\n",
    "phi = 350\n",
    "glicko = Glicko2(mu=mu, phi=phi, tau=tau, sigma=sigma)\n",
    "# glicko.create_rating(mu=mu, phi=phi)\n",
    "# glicko.rate(old_rating, series)\n",
    "# series is a list of ([result, opponent_rating]) tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188ef5c-447d-445f-9388-b285ccb13cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# which period do you want to use as series? Day, Week, Month...\n",
    "# remember to have provisional ratings within the rating period and only update the \"real rating\" after the series end!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a068d05-7083-4c98-99e0-bca99c039239",
   "metadata": {},
   "source": [
    "#### pseudocode (don't read if you want a challenge)\n",
    "\n",
    "    for period_name, series in df.groupby(period):\n",
    "    \n",
    "        provisional_ratings = {}\n",
    "    \n",
    "        for team in all_teams:\n",
    "    \n",
    "            - get all matches the team has played in this series\n",
    "               \n",
    "            - if so, rewrite team results in the format taken by algorithm\n",
    "        \n",
    "            - calculate new provisional rating of the team using this series and save it\n",
    "    \n",
    "        after doing this for all teams, the provisional ratings you calculated become the real ratings of the teams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa31d20-84e2-4fcd-a145-f58da1aac85a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b7f4f1-d00b-4a4f-8dd2-32ce408d3241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# win predictions\n",
    "def predict_glicko_winner(r1: Glicko2.rating, r2: Glicko2.rating) -> float:\n",
    "    r1_g2 = glicko.scale_down(r1)\n",
    "    r2_g2 = glicko.scale_down(r2)\n",
    "    # equivalent to calling Glicko2.reduce_impact\n",
    "    g = 1 / np.sqrt(1+3*r1_g2.phi **2 / np.pi**2)\n",
    "    # equivalent to calling Glicko2.expect_score\n",
    "    E = 1 / (1 + np.exp(-g * (r1_g2.mu - r2_g2.mu)))\n",
    "    return E\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b0bd65-1f80-4489-833a-0fc04ba923d5",
   "metadata": {},
   "source": [
    "## Trueskill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b089e4-29c5-4b0b-882a-0a3a98da0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mu = 25\n",
    "sigma = 8.33\n",
    "p_draw = 0\n",
    "beta = 8.333 / 2\n",
    "ts = TrueSkill(backend='scipy')\n",
    "# ts.Rating(mu=mu, sigma=sigma)\n",
    "# ts.rate_1vs1(winner_rating, loser_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2e6c02-a0a8-4b3b-bf52-00fe97d06f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# win predictions\n",
    "# TrueSkill provides a function (quality()) to calculate a draw probability between arbitrary ratings (because matchmaking is the goal)\n",
    "# But thereâ€™s no function for a win probability.\n",
    "\n",
    "def ts_win_probability(r1: TrueSkill.rating, r2: TrueSkill.rating):\n",
    "    delta_mu = r1.mu - r2.mu\n",
    "    sigmas = np.array([r1.sigma, r2.sigma, np.sqrt(2)*beta])\n",
    "    p_1 = norm.cdf(delta_mu / np.linalg.norm(sigmas))\n",
    "    p_2 = 1 - p_1\n",
    "    ## correct for draws\n",
    "    #p_1 -= 0.5 * p_draw\n",
    "    #p_2 -= 0.5 * p_draw\n",
    "    #if p_1 < 0:\n",
    "    #    p_1 = 0\n",
    "    #    p_2 += np.abs(p_1)\n",
    "    #if p_2 < 0:\n",
    "    #    p2 = 0\n",
    "    #    p_1 += np.abs(p_2)\n",
    "    return p_1, p_2, p_draw\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datalift-workshop",
   "language": "python",
   "name": "datalift-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
